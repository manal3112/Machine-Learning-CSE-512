\documentclass{article}
\usepackage{tabularx,fullpage,url}
\usepackage[top=1in, bottom=1in, left=.5in, right=.75in]{geometry}
\usepackage{amsmath,amssymb,graphicx,amsthm,xparse, color, mathrsfs} 
\usepackage{ epstopdf, fullpage}

\usepackage{xifthen}
\usepackage[ruled,vlined]{algorithm2e}

\newcommand{\mypagebreak}{\begin{center}
		\noindent\makebox[\linewidth]{\rule{7.5in}{1pt}}
	\end{center}}
\bibliographystyle{siam}
\newcommand{\minimize}[1]{\underset{#1}{\text{minimize}}}
\newcommand{\maximize}[1]{\underset{#1}{\text{maximize}}}
\newcommand{\mini}[1]{\underset{#1}{\text{min}}}
\newcommand{\argmin}[1]{\underset{#1}{\text{argmin}}}
\newcommand{\st}{\text{subject to}}
\newcommand{\rank}{\textbf{rank}}
\newcommand{\epi}{\mathbf{epi}}

\newcommand{\diag}{\textbf{diag}}
\newcommand{\mb}{\mathbf}
\newcommand{\R}{\mathbb R}
\newcommand{\mle}{\mathbf{MLE}}
\newcommand{\map}{\mathbf{MAP}}
\newcommand{\bE}{\mathbb E}
\newcommand{\mL}{\mathcal L}
\newcommand{\mH}{\mathcal H}
\newcommand{\mN}{\mathcal N}
\newcommand{\mD}{\mathcal D}
\newcommand{\mC}{\mathcal C}

\newcommand{\mS}{\mathcal S}
\newcommand{\tr}{\mathbf{tr}}
\newcommand{\mrm}{\mathrm}
\newcommand{\proj}{\mathbf{proj}}
\newcommand{\prox}{\mathbf{prox}}
\newcommand{\sign}{\mathbf{sign}}
\newcommand{\range}{\mathbf{range}}
\newcommand{\var}{\mathbf{var}}
\newcommand{\vnull}{\mathbf{null}}
\newcommand{\pr}{\mathbf{Pr}}
\newcommand{\find}{\mathbf{find}}
\newcommand{\argmax}[1]{\underset{#1}{\mathrm{argmax}}}
\newcommand{\subjto}{\mathrm{subject~to}}

\newcommand{\bmat}{\left[\begin{matrix}}
\newcommand{\emat}{\end{matrix}\right]}

\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}

\newcommand{\gray}[1]{\textcolor{lightgray}{#1}}


\newcommand{\ifequals}[3]{\ifthenelse{\equal{#1}{#2}}{#3}{}}
\newcommand{\case}[2]{#1 #2} 
\newenvironment{switch}[1]{\renewcommand{\case}{\ifequals{#1}}}{}






\newcommand{\showpoints}[1]{\textbf{(#1 pts)} }

\begin{document}
{\Large\textbf{CSE 512: Homework 5 \hfill
Due Nov. 19}}


\mypagebreak


\begin{enumerate}


\item \textbf{Directed graphical models and probability inference}
I have 4 tops: a red sweater, a blue T-shirt, a green hoodie, and a white tank top.
I need your help to decide what to wear.

\begin{enumerate}

\item \showpoints{1}  I decide what to wear based on four factors:
\begin{itemize}
\item if it's raining
\item if I want to take a walk outside
\item if I feel sick
\item the day of the week it is
\end{itemize}

%\begin{center}
%\begin{tabular}{l|llll}
%& red sweater & blue T-shirt & green hoodie & white tank top\\\hline
%raining & 40\% & 10\% & 40\% &10\%\\
%walking outside & 35\% & 5\% & 60\% & 0\%\\
%I feel sick & 5\% & 5 \% & 80\% & 10\%\\
%It's Monday & 80\% & 5\% & 10\% & 5\%
%\end{tabular}
%\end{center}

Using the Naive Bayes assumption, draw a graphical model that indicates how I will make a decision of what to wear each day.


\item \showpoints{1}  To be more specific, 
\begin{itemize}
\item I only wear the green hoodie when I walk outside, regardless of all other factors. 
\item If I feel sick, I will walk outside 10\% of the time. If I feel well, I will walk outside 60\% of the time.
\item When it rains, I feel sick 70\% of the time; otherwise, I feel sick 15\% of the time.
\end{itemize}
Draw a corresponding graphical model for determining whether I wear a green hoodie. Given that it is raining, infer the probability that I am wearing a green hoodie.





\item \showpoints{1}  The probability that I wear a tank top, independently of all the other clothes, is 75\% if it's raining and 25\% if it's not raining.
\begin{itemize}
\item
Today is Monday and it is raining.

\item 
The probability that it will rain, given that the previous day rained, is 70\%. The probability that it will rain, given that the previous day did not rain, is 10\%. 

\end{itemize}
Draw a graphical model predicting whether I will wear a tank top on Wednesday, and calculate this probability.



\end{enumerate}






\item \textbf{Clustering}\showpoints{3} 



\begin{itemize}
\item Open the python notebook \texttt{mnist\_dimred\_clustering.ipynb}, and load the MNIST data by running the first cell. Don't change the way I've formatted it, or the checksums won't work.

\item Fill in the function for determining the Euclidean distance between any sample point and the entire training data.
From the print function, you should get a value of \texttt{160239119987.1912}.


\item Using this function, code up a K-Means method, using an initialization of 
\begin{verbatim}
mu = X[:K,:]
\end{verbatim}
where $K =10$ is the number of clusters. Run this for 10 iterations.

\item Define the \emph{purity} of a set $\mS$ as the following fraction:
\[
\text{class\_purity}(\mathcal S) = \max_i\frac{|\{y_j = i, j\in\mS\}|}{|\mS|}
\]
that is, it is the size of the largest same-label subset of $\mathcal S$ divided by the total size of $\mathcal S$.
Report the purity of your clustering method after running k-means \emph{on only the first 25 datapoints}, up to 3 digits after the decimal. Plot also the clustering result.





\item Now, put the above section in a nice function, as we will use it again and again to evaluate future embeddings. 

\item At this point, we should also  de-mean the data, as it will improve the embedding performances:
\begin{verbatim}
X = X - np.outer(np.ones(X.shape[0]),np.mean(X,axis=0)).
\end{verbatim}


\begin{itemize}
\item Using \texttt{np.linalg.svd}, implement PCA, and reduce the data dimension to $d = $ 10, 100, and 500.  Re-run K-Means to get new class memberships.
(You may want to enact the option \texttt{full\_matrices = False} when computing the SVD.)




\item Use random hashing (as promoted by the JL lemma) and reduce the feature dimension to 10,100,500 dimensions, and cluster with MNIST. 






\item Use Isomap, LLE, or spectral embeddings to reduce dimension to 10,100,500, and cluster with MNIST. You only need to do one of the three, but you do need to code it up from scratch. When constructing the nearest neighbor graph, use Euclidean distance, and pick a threshold that seems reasonable to you. (Can be distance thresholding or nearest number of neighbors.) For isomap, you may find \texttt{scipy.sparse.csgraph.shortest\_path} useful, if you can create the appropriate CSR formatted adjacency matrix. For all other hyperparameters, make a choice that seems reasonable to you.




\item Use sklearn's t-SNE to  to reduce dimension to 1,2,3, and cluster with MNIST. In addition, plot something that shows the clustering effect, either in 1,2, or 3-D space. Keep most default hypermarameter settings to make your answers comparable.

\end{itemize}

\item For these three situations,  return the purity for each method and dimension size. 

\item Comment on the effect of the different dimensionality reduction schemes in the MNIST clustering task. What are the tradeoffs, in terms of performance and computational complexity?


\end{itemize}





\item \textbf{Hidden Markov Model spellchecker}\showpoints{4} 
In this exercise we will make a spell-checker using a HMM. To do this, download \texttt{alice\_nlp\_release.ipynb} and follow the instructions.

\begin{itemize}
\item Read through the first two blocks to get an idea of what the task is. The idea is to go through the corrupted corpus, identify words which have probably been corrupted, and correct them probabilistically. 

\item In the 4th box, fill in the functions to construct the word probabilities (weighted frequencies in uncorrupted corpus) and transition matrix (which gives Pr(word $|$ prev word)). If done correctly, the lines printed out should read 

\begin{verbatim}
prob. of "alice" 0.014548615047424706
prob. of "queen" 0.002569625514869818
prob. of "chapter" 0.0009069266523069947
\end{verbatim}

with smoothing

\begin{verbatim}
prob. of "the alice" 0.00025406504065040653
prob. of "the queen" 0.016514227642276422
prob. of "the chapter" 0.012957317073170731
\end{verbatim}

no smoothing

\begin{verbatim}
prob. of "the alice" 0.0
prob. of "the queen" 0.03968253968253968
prob. of "the chapter" 0.0
prob. of "the hatter" 0.031135531135531136
\end{verbatim}

\item In the 5th box, fill in the function for computing the emission probability. The first 10 words closest to Alice should be 

\begin{verbatim}
['abide', 'alice', 'above', 'voice', 'alive', 'twice', 'thick', 'dance', 'stick', 'prize']
\end{verbatim}

\item Construct and run your Hidden Markov Model spell checker using the functions computed for the prior probabilities, emission probabilities, and transition probabilities. List some words whose spelling was corrected correctly, and some examples where the spell-correcter did not work as expected. Report the recovery rate of the ``fixed" corpus.



\end{itemize}







\end{enumerate}


\newpage
\section*{Challenge!}


\begin{enumerate}

\item\textbf{Correlated mixture of sequential experts.} 

I want to buy a yacht, but I'm not sure if it's a good idea given the economy. So, I decide to question $m$ consultants. Each consultant has more-or-less the same qualifications, and they come in one at a time.


The first consultant comes in my office. I ask, ``Should I buy a yacht?" She says yes with probability $p$.

On her way out the building, she meets the second consultant. They chat briefly, and she leaves, he comes, and the process repeats. Each time, I ask the consultant if I should buy a yacht, and receive ``yes" with probability $p$; each time, the consultant chats briefly with the next consultant. \emph{However}, the answers now are \emph{not} i.i.d., but rather each expert's answer is correlated with the answer of experts he/she chatted with in the lobby.

At the end of the day, I have met with $m$ consultants. I will make a decision whether to buy a yacht based on majority rule. We will now calculate the probability that I will buy a yacht.


\begin{enumerate}
\item We model the answer of each consultant as $Y_i = 1$ if the $i$th consultant recommended ``yes", and $Y_i = -1$ otherwise. 
Show that if distribution 
\[
\pr(Y_1 = 1) = p, \qquad \pr(Y_i = 1 | Y_{i-1} = 1) = c + p - cp, \qquad \pr(Y_i=-1|Y_{i-1}=-1) = cp-p+1
\]
then $\pr(Y_i = 1) = p$ for all $i$. 




\item The Pearson correlation coefficient between a random variable $U$ and $V$ can be expressed as 
\[
\mathbf{corr}(U,V) = \frac{\bE[UV]-\bE[U]\bE[V]}{\sqrt{\var(U)\var(V)}}.
\]
Show that the correlations between each pair of sequential experts $\mathbf{corr}(Y_i, Y_{i-1}) = c$,

Hint: Go ahead and use a symbolic calculator, like WolframAlpha, to simplify messy expressions.



\item For $m = 3$, what is the probability that I will buy a yacht, in terms of $P_{11} = \pr(Y_i=1|Y_{i-1}=1)$,$P_{00} = \pr(Y_i=-1|Y_{i-1}=-1)$ and $p$?



\item 
\textbf{Code.}  Compute exactly the probability that I will buy a yacht, for $m = 10$, in Python or MATLAB. 
Generate a plot that shows the probability that I will buy a yacht, sweeping $c\in [-1,1]$. (Note that there are cases where $p$ and $c$ are an infeasible pair, and can be detected when probabilities are not in the range (0,1)--these cases should not be plotted.) Do this for several interesting values of $p$.
Comment on how the decision changes as a function of $c$, $p$, and $m$.



\item Simulate the sequential advisers, and give a numerical estimate of what my decision will be if $m = 25$ and $m = 100$. Generate a similar plot, using these numerical estimates of $\pr(\text{I will buy a yacht})$. Use your own discretion to decide how many trials you need, and what values of $c$ and $p$ are useful.



\end{enumerate}


\end{enumerate}



\end{document}